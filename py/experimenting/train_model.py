import numpy as np
import pathlib
import random
from functools import reduce
import os, sys
from PIL import Image
from keras.utils import np_utils

def loadimg(img_path, reshape = None):
    photo = Image.open(img_path) #your image
    if(reshape != None):
        photo = photo.resize(reshape, Image.ANTIALIAS)
    #photo = photo.convert('RGB')
    result = []
    width = photo.size[0] #define W and H
    height = photo.size[1]
    for y in range(0, height): #each pixel has coordinates
        row = []
        for x in range(0, width):
            col = []
            RGB = photo.getpixel((x,y))
            R,G,B = RGB  #now you can use the RGB value
            col =  [[R,G,B]]
            row = row + col
        result = result + [row]
    return result


def getdata(dataset_name, reshape=None):

    (x, y) = loadDS(os.path.abspath('..\\datasets\\'+dataset_name+'\\train\\'), reshape)
    (xt, yt) = loadDS(os.path.abspath('..\\datasets\\'+dataset_name+'\\test\\'), reshape)
    x = np.array(x, dtype=np.uint8)
    y = list(map(lambda i : [i], y))
    y = np.array(y, dtype=np.uint8)

    xt = np.array(xt, dtype=np.uint8)
    yt = list(map(lambda i : [i], yt))
    yt = np.array(yt, dtype=np.int32)

    return ((x, y),(xt, yt))
    
def loadDS(folder_url, reshape = None):
    data_root = folder_url
    data_root = pathlib.Path(data_root)
    #print(data_root)

    all_image_paths = list(data_root.glob('*/*'))
    all_image_paths = [str(path) for path in all_image_paths]
    random.shuffle(all_image_paths)

    image_count = len(all_image_paths)
    #print(image_count)
    label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())
    #print(label_names)
    label_to_index = dict((name, index) for index,name in enumerate(label_names))
    label_to_index
    all_image_labels = [label_to_index[pathlib.Path(path).parent.name]
                    for path in all_image_paths]

#[add(x, 2) for x in [1, 2, 3]]
    #result = list( map(loadimg, all_image_paths) )
    result = [loadimg(x, reshape) for x in all_image_paths]

    return (result, all_image_labels)



def train_classification_model(model, dataset_name, batch_size, nb_classes, nb_epoch, img_rows, img_cols, img_channels, data_augmentation = False, reshapeTo = None):
    #batch_size = 32
    #nb_classes = 10
    #nb_epoch = 200
    #data_augmentation = False

    # input image dimensions
    #img_rows, img_cols = 32, 32
    # The CIFAR10 images are RGB.
    #img_channels = 3

    # The data, shuffled and split between train and test sets:
    (X_train, y_train), (X_test, y_test) =  getdata(dataset_name, reshapeTo) # cifar10.load_data()

    #(X_train1, y_train1), (X_test1, y_test1)= getdata()

    #print('X_train shape:', X_train.shape)
    #print(X_train.shape[0], 'train samples')
    #print(X_test.shape[0], 'test samples')

    # Convert class vectors to binary class matrices.
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)


    # Let's train the model using RMSprop
    model.compile(loss='categorical_crossentropy',
                optimizer='rmsprop',
                metrics=['accuracy'])

    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255

    if not data_augmentation:
        print('Not using data augmentation.')
        model.fit(X_train, Y_train,
                batch_size=batch_size,
                nb_epoch=nb_epoch,
                validation_data=(X_test, Y_test),
                shuffle=True)
    else:
        print('Using real-time data augmentation.')
        # This will do preprocessing and realtime data augmentation:
        datagen = ImageDataGenerator(
            featurewise_center=False,  # set input mean to 0 over the dataset
            samplewise_center=False,  # set each sample mean to 0
            featurewise_std_normalization=False,  # divide inputs by std of the dataset
            samplewise_std_normalization=False,  # divide each input by its std
            zca_whitening=False,  # apply ZCA whitening
            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
            horizontal_flip=True,  # randomly flip images
            vertical_flip=False)  # randomly flip images

        # Compute quantities required for featurewise normalization
        # (std, mean, and principal components if ZCA whitening is applied).
        datagen.fit(X_train)

        # Fit the model on the batches generated by datagen.flow().
        model.fit_generator(datagen.flow(X_train, Y_train,
                                        batch_size=batch_size),
                            samples_per_epoch=X_train.shape[0],
                            nb_epoch=nb_epoch,
                            validation_data=(X_test, Y_test))


from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
def sample_model(img_rows,img_cols, img_channels):
    
    model = Sequential()

    model.add(Convolution2D(32, 3, 3, border_mode='same',
                            input_shape=(img_rows,img_cols, img_channels)))
    model.add(Activation('relu'))
    model.add(Convolution2D(32, 3, 3))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Convolution2D(64, 3, 3, border_mode='same'))
    model.add(Activation('relu'))
    model.add(Convolution2D(64, 3, 3))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(nb_classes))
    model.add(Activation('softmax'))

    return model

#batch_size = 5
#nb_classes = 10
#nb_epoch = 200
#img_rows = 32
#img_cols = 32
#img_channels = 3
#data_augmentation = False
#train_classification_model(sample_model(img_rows,img_cols, img_channels), 'cifar10', batch_size, nb_classes, nb_epoch, img_rows, img_cols, img_channels, data_augmentation)

#dataset_name = 'cifar10'
#result = getdata(dataset_name)
#ff = 1


print('FIN')